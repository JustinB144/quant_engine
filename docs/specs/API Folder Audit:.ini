API Folder Audit:
Critical Findings

Unauthenticated mutation/compute endpoints are exposed. Any caller can start training/backtests/autopilot jobs and patch runtime config with no authz guard. References: main.py:106, routers/model_lab.py:65, routers/backtests.py:77, routers/autopilot.py:73, routers/config_mgmt.py:134.
SQL-injection risk in Kalshi query construction: user input is interpolated directly into where= SQL text. Reference: services/kalshi_service.py:45.
Job cancellation is not real cancellation for thread-backed work. task.cancel() cancels the coroutine wrapper, but asyncio.to_thread(...) work can keep running and still produce side effects. References: jobs/runner.py:53, jobs/runner.py:83.
Router import failures are silently swallowed, so production can boot with missing API surface and only a warning log. References: routers/init.py:36, routers/init.py:40.
High Findings

CORS is permissive-by-default (*) while credentials are enabled; this is a dangerous default posture for non-internal deployments. References: config.py:32, main.py:121.
Runtime config coercion is unsafe for booleans (bool("false") == True), causing incorrect live behavior on PATCH. References: config.py:68, config.py:70.
Runtime config patching lacks semantic validation (ranges/invariants) and persistence guarantees; invalid-but-coercible values can be accepted. References: config.py:57, config.py:73, routers/config_mgmt.py:140.
Diagnostics and risk endpoints return placeholder/schema data as successful responses, which can mislead operators into trusting non-live analytics. References: routers/diagnostics.py:34, routers/risk.py:34.
HealthService snapshots are persisted on every detailed-health request, so observability calls mutate health history and can distort trends/alerts. References: services/health_service.py:201, services/health_service.py:1945.
Health checks execute heavy full-cache scans synchronously in request path; this is a latency and availability risk under load. References: services/health_service.py:1099, services/health_service.py:1191.
Orchestrator silently drops prediction/backtest failures (pass), hiding data/model defects and making output quality non-auditable. References: orchestrator.py:248, orchestrator.py:315.
Job submission is effectively unbounded (create_task per request, semaphore only bounds execution), enabling memory pressure/queue blow-up. References: jobs/runner.py:35, jobs/runner.py:21.
model_age endpoint can fail on timezone-aware training timestamps due naive/aware datetime subtraction mismatch. References: routers/system_health.py:76, routers/system_health.py:77.
A sync SQLite-heavy call is executed directly inside async route handler, blocking event loop. Reference: routers/system_health.py:51.
Cache manager is unsynchronized mutable shared state; concurrent access can race and cached objects are returned by reference (caller mutation risk). References: cache/manager.py:25, cache/manager.py:36, cache/manager.py:55.
A/B test trade persistence uses unsanitized variant names in filesystem paths, allowing path traversal if names are externally sourced. References: ab_testing.py:592, ab_testing.py:712, ab_testing.py:784.
A/B statistical implementation is not defensible as written: Newey-West variance is computed on concatenated centered arms rather than a proper paired/effective process; block bootstrap pooling/slicing also weakens inference validity. References: ab_testing.py:292, ab_testing.py:303, ab_testing.py:265.
Medium Findings

API contract is inconsistent: some routes return raw dicts instead of ApiResponse envelope. References: routers/risk.py:20, routers/diagnostics.py:20.
regime_trade_policy is injected into meta in signals route but is not a field in ResponseMeta; it is silently discarded. References: routers/signals.py:26, schemas/envelope.py:12.
Backtest-related cache is invalidated when job is queued, not when successful completion occurs; this can produce churn and stale/empty recomputation windows. Reference: routers/backtests.py:88.
data_explorer batch indicators endpoint uses POST but takes all inputs as query params; this is semantically inconsistent and hard to evolve safely. References: routers/data_explorer.py:461, routers/data_explorer.py:465.
Cached parquet selection logic can pick the first matching file rather than the best/newest match, leading to non-deterministic data quality. References: routers/data_explorer.py:55, routers/data_explorer.py:56.
Health trend detector thresholds (±0.5 slope per observation on 0-100 scale) are too coarse and likely classify subtle drifts as stable. References: services/health_service.py:2103, services/health_service.py:2105.
Feature-stability history path is hardcoded relative (results/...) instead of config-driven absolute pathing; portability risk. Reference: services/health_service.py:1817.
Log buffer handler is attached at import time, which can duplicate handlers across reloads and skew log volume. References: routers/logs.py:30, routers/logs.py:32.
Mutable defaults are used throughout schema models ({}/[]); while Pydantic often mitigates this, it remains fragile style and increases maintenance risk. References: schemas/autopilot.py:21, schemas/dashboard.py:29, schemas/system_health.py:13, schemas/signals.py:25.
Fundamental/Conceptual Critique

The health subsystem is a monolith with mixed responsibilities (scoring logic, storage, alerting, DB schema, trend analytics) in a single ~3k LOC module, which harms testability and reliability boundaries. Reference: services/health_service.py:1.
The API layer is strongly coupled to filesystem artifacts (results/, parquet, json) and runtime global state (quant_engine.config mutation), making behavior environment-dependent and hard to reason about under concurrent usage.
Statistical claims are often stronger than implementation fidelity (especially A/B and some health checks), creating epistemic risk: outputs may look rigorous while inference assumptions are not enforced.
Assumptions / Scope

I audited every source file in /Users/justinblaise/Documents/quant_engine/api (49 files, 10,188 LOC) and traced internal references within this folder.
External modules under quant_engine.* outside this folder were not audited in this pass, so findings are strictly based on code here.
No code was changed.




Autopilot Folder Audit:
Audit Findings (Ordered by Severity)

[P0] Drawdown state is mutated during sizing, not just once per cycle.
Evidence: paper_trader.py:709 calls self._dd_controller.update(0.0) inside _position_size_pct (called per candidate entry). In the controller, update() increments day counters and appends PnL history drawdown.py:102, drawdown.py:104.
Impact: drawdown regime/timers can drift based on how many symbols were evaluated, not actual time/PnL.

[P0] Portfolio optimizer output is computed but not used by execution sizing.
Evidence: optimizer weights are written in engine.py:1651-engine.py:1658, but PaperTrader sizes only from Kelly/risk/confidence path and confidence_weight paper_trader.py:1068-paper_trader.py:1092.
Impact: optimization work is effectively dead for live paper allocations.

[P0] A/B test Kelly override is calculated but never applied.
Evidence: eff_kelly_fraction is set from variant config paper_trader.py:1031, paper_trader.py:1052-paper_trader.py:1054, but _position_size_pct uses self.kelly_fraction paper_trader.py:707 and never receives eff_kelly_fraction.
Impact: A/B tests claiming Kelly differences are invalid.

[P0] Meta-label retraining leaks across assets due global concatenation with rolling features.
Evidence: retraining concatenates all assets into one series with prefixed string index engine.py:1125-engine.py:1138, then calls feature builder engine.py:1155. Feature builder uses rolling windows directly on that series meta_labeler.py:122-meta_labeler.py:136.
Impact: boundary rows can use adjacent asset history; training signal quality is contaminated.

[P1] State persistence is non-atomic and unguarded against partial/corrupt JSON.
Evidence: direct overwrite writes in registry.py:48-registry.py:49, paper_trader.py:249-paper_trader.py:250; direct json.load without decode recovery in registry.py:43, paper_trader.py:236.
Impact: crash on interrupted writes; no file lock for concurrent cycles.

[P1] Configured transition-drawdown gate is dead code.
Evidence: imported and stored promotion_gate.py:35, promotion_gate.py:90, promotion_gate.py:120, but never used in decision logic.
Impact: intended risk control is absent while config/docs imply it is active.

[P1] Turnover anchoring uses stale entry price because last_price is never persisted.
Evidence: optimizer baseline uses last_price fallback to entry_price engine.py:1284; no last_price updates exist in paper trader state writes (only reference found is that line).
Impact: turnover penalty can be materially wrong, especially for drifting positions.

[P1] Cost feedback/health signals are biased by clipping favorable slippage to zero.
Evidence: recorded actual_cost_bps is forced nonnegative paper_trader.py:498, paper_trader.py:509. Cost calibrator computes surprise as predicted - actual cost_calibrator.py:361.
Impact: no representation of price improvement; systematic upward bias in measured cost surprise.

[P1] Promotion registry replacement model can cause strategy churn.
Evidence: active set is replaced wholesale each cycle registry.py:92, sourced only from current passers/top-N registry.py:65-registry.py:68.
Impact: incumbents disappear immediately if omitted from current candidate set or if candidate truncation changes.

[P2] Uncertainty feature implementation does not match its own stated methodology.
Evidence: doc says rolling uncertainty behavior engine.py:1187-engine.py:1192, but code computes cross-sectional deviation at one snapshot engine.py:1197-engine.py:1206.
Impact: signal-uncertainty semantics are inconsistent and hard to trust.

[P2] Meta-label filter uses global config threshold instead of model’s loaded threshold.
Evidence: filter threshold sourced from META_LABELING_CONFIDENCE_THRESHOLD engine.py:1001, while model persists/loads its own threshold meta_labeler.py:397, meta_labeler.py:439-meta_labeler.py:441.
Impact: runtime behavior can diverge from trained model contract.

[P2] MetaLabelingModel.save(filepath=...) still writes a second hardcoded file.
Evidence: explicit path write meta_labeler.py:401, then unconditional write to meta_labeler_current.joblib meta_labeler.py:404-meta_labeler.py:406.
Impact: hidden side effect and permission sensitivity in constrained environments.

[P2] Critical integrations are heavily fail-open with broad exception swallowing.
Evidence: health/A-B/cost paths in paper_trader.py:175, paper_trader.py:199, paper_trader.py:214, paper_trader.py:543, paper_trader.py:1234, paper_trader.py:1249; engine paths in engine.py:692, engine.py:1778, engine.py:1826.
Impact: silent degradation is likely; observability is weak.

[P2] Private API coupling increases fragility.
Evidence: engine calls paper_trader._load_state() engine.py:1272; paper trader calls self._ab_registry._save() paper_trader.py:228.
Impact: changes in internal methods can break runtime paths without interface guarantees.

[P3] StrategyAllocator is effectively disconnected from runtime orchestration.
Evidence: module claims engine integration strategy_allocator.py:12-strategy_allocator.py:15, but no production references in autopilot flow (only docs/tests).
Impact: documented regime-aware allocation path is not actually in the live cycle.

[P3] Hygiene issues: stale docs and unused imports.
Evidence: README metadata/line counts are stale relative to current files README.md:9-README.md:25; unused import strategy_signal_returns engine.py:31; unused field import strategy_allocator.py:17.
Impact: maintainability and trust in docs degrade.

[P3] n_folds in meta-labeler is configured but not used in training path.
Evidence: constructor stores n_folds meta_labeler.py:85-meta_labeler.py:90, but train() uses fixed 90/10 split and never references it meta_labeler.py:256-meta_labeler.py:260.
Impact: config surface is misleading.

[P3] Fundamental complexity concentration in single orchestrator/file.
Evidence: one class controls data load, model train/retrain, validation, promotion, optimization, health tracking, and paper execution in a 1,827-line file engine.py:147-engine.py:1827.
Impact: high cognitive load, hard isolation testing, and regression risk from unrelated edits.

Verification Run

Syntax compile pass succeeded with local pycache redirection: PYTHONPYCACHEPREFIX=/tmp/pycache python3 -m compileall -q ..
Targeted autopilot tests: 69 passed, 1 failed (test_save_and_load_roundtrip), failure occurs because save(path) still writes to fixed meta_labeler_current.joblib path, which is not writable in this sandboxed run.
Scope Notes

Audited all tracked files in this folder (README.md + 8 Python modules, 4,443 lines total).
Cross-checked referenced source where needed for non-guess findings (drawdown controller, cost calibrator, optimizer, backtester, config).
No code was modified.




Backtests Folder Audit:

Findings (ordered by severity)

[P0] Portfolio-level performance math is internally inconsistent and can materially overstate results.
In simple mode, trade returns are treated as if every trade uses full capital (returns = t.net_return), while equity drawdown is computed from position-weighted daily returns. This mixes two different return models in one report.
Refs: engine.py:1889, engine.py:1903, engine.py:1943, engine.py:2108

[P0] Cost calibration is fed impact-only costs, but calibrator assumes total slippage and subtracts spread again.
Backtester records impact_bps as “realized_cost_bps”; calibrator docs/logic expect spread+impact and subtract half-spread, biasing calibrated coefficients downward.
Refs: engine.py:487, engine.py:619, cost_calibrator.py:175, cost_calibrator.py:238

[P0] Package import contract is broken in dependency config, which blocks test execution.
config.py uses absolute import from config_structured import ..., which fails under package import (quant_engine.config). This caused collection failures across backtest-related tests.
Ref: config.py:29

[P1] Almgren-Chriss implementation has structural modeling issues.
Trajectory shape uses a formula typically for remaining inventory, then normalizes as trade list; participation metric is also scaled against full-day volume rather than interval volume.
Refs: optimal_execution.py:93, optimal_execution.py:191

[P1] TCA “predicted vs realized” compares different quantities (impact-only vs spread+impact).
Predicted arrays use entry_impact_bps/exit_impact_bps, realized slippage includes spread + impact from fill-vs-reference; correlation/RMSE are therefore biased by construction.
Refs: engine.py:2301, engine.py:2315, engine.py:2294

[P1] Execution urgency pullback reduces quantity but does not recompute participation/impact.
participation_rate is computed before urgency-based size reduction and returned stale; this contaminates downstream calibration and diagnostics.
Refs: execution.py:658, execution.py:759, execution.py:788

[P1] calibrate_cost_model() can infer negative spread values.
For negative slippage with participation present, est_spread can go negative due min(max(...), slippage_bps).
Ref: execution.py:901

[P1] Edge-cost gate uses base position size, not eventual adjusted size.
Trade gating is done on position_size_pct not the uncertainty- or risk-adjusted size ultimately executed.
Refs: engine.py:1052, engine.py:1106, engine.py:1631, engine.py:1739

[P1] quick_survivorship_check() accepts predictions but does not use it.
The function’s contract claims prediction coverage checks, but implementation only compares universe sets/bars.
Refs: survivorship_comparison.py:125, survivorship_comparison.py:143

[P1] Capacity estimate uses first trade size as representative size.
This is fragile when position sizes vary materially.
Ref: advanced_validation.py:426

[P2] compute_cost_surprise_by_segment() doc promises "_all" aggregate but implementation does not return it.
Refs: cost_calibrator.py:396, cost_calibrator.py:409

[P2] Null/stress result fields exist in BacktestResult but are not populated in engine run path.
This creates API expectations that aren’t fulfilled unless caller patches results manually.
Refs: engine.py:141, engine.py:2019

[P2] Stop-loss evaluation uses full-bar high/low/close during same-bar decision loop.
Depending on simulation assumptions, this can embed intrabar look-ahead in stop decisions.
Refs: engine.py:1303, engine.py:1314, engine.py:1332

[P2] Advanced validation formulas are simplified/non-canonical in places and use sentinel outputs.
deflated_sharpe=-999 sentinel and PBO “logit” from Sharpe deltas are implementation choices that diverge from standard reporting conventions.
Refs: advanced_validation.py:143, advanced_validation.py:266

[P2] Broad exception swallowing can hide systematic breakage.
Example: shock-vector precompute failures are logged and silently downgraded to no-structural-state execution.
Ref: engine.py:883

[P3] README metadata is stale vs actual codebase size/shape.
Documented module/class/function/LOC counts do not match current folder contents.
Ref: README.md:9

[P3] Maintainability complexity is very high in core paths.
Ruff complexity checks flag critical functions (run, _process_signals_risk_managed, simulate, etc.) as over-threshold, increasing regression risk.
Refs: engine.py:822, engine.py:1173, execution.py:529, validation.py:197

[P3] Hygiene drift is substantial (unused imports, stale f-strings, etc.).
Not correctness-critical alone, but it increases noise and weakens signal for real defects during review/linting.

Open questions / assumptions

Are performance metrics intended to represent portfolio returns or per-trade unit returns? Current implementation mixes both models.
Should cost calibration ingest total slippage or impact-only? Current interfaces/documentation disagree.
Is strict package importability (quant_engine.*) required in CI? Current config import path suggests this is currently broken.
Audit execution summary

Read every file in /Users/justinblaise/Documents/quant_engine/backtest line-by-line (12 files, 7,048 lines total).
Resolved and inspected all non-stdlib references used by this folder in source (config, regime, risk, validation.preconditions).
Read-only checks run:
py_compile on all backtest modules: pass.
Targeted pytest suites: blocked at collection by ModuleNotFoundError: config_structured and cache permission warnings.
ruff:
full check: 239 issues.
runtime-oriented (--select F,B,E9): 22 issues.
complexity (--select C901): 12 over-complex functions.




Evalutions Folder Audit:

Findings (Ordered by Severity)

[Critical] analyze_calibration crashes on mismatched confidence_scores length before fallback logic runs.
Code slices confidence_scores then boolean-indexes with a different-length mask, raising IndexError.
Refs: calibration_analysis.py:74, calibration_analysis.py:75, calibration_analysis.py:77.

[Critical] Engine is fail-open: subsystem exceptions are swallowed and evaluation continues with partial/quietly missing diagnostics.
This can produce reports that look complete while sections silently failed.
Refs: engine.py:236, engine.py:253, engine.py:308, engine.py:339, engine.py:366, engine.py:454.

[High] Regime/uncertainty/transition slices never compute IC, even though IC is a core predictive metric.
compute_slice_metrics supports predictions, but engine passes None for all slice calls, forcing ic=0.0 in slice outputs.
Refs: engine.py:215, engine.py:223, engine.py:233, engine.py:250, metrics.py:27.

[High] Annualized return formula is statistically wrong for volatile returns.
Uses (1 + mean_return)^252 - 1 instead of geometric compounding / CAGR from path-level returns.
Ref: metrics.py:65.

[High] Decile binning is not true quantile assignment and produces imbalanced buckets.
Integer cast of rank * n_quantiles biases edge bins; observed counts with perfectly ranked 100 points were [9,10,...,11] instead of even deciles.
Refs: metrics.py:183, metrics.py:184.

[High] pnl_concentration reports n_trades=0 when total PnL nets to ~0, even if trades exist.
This is data loss in the returned summary object.
Refs: fragility.py:69, fragility.py:70.

[High] Recovery-time indexing can be wrong when returns contain NaNs.
Values are NaN-filtered by position, but trough indices are later mapped onto the original unfiltered index.
Refs: fragility.py:209, fragility.py:210, fragility.py:242.

[High] Config/implementation drift on risk thresholds and lookbacks.
Engine imports active config constants but does not apply them in decision points; parts are hardcoded elsewhere.
Refs: engine.py:34, engine.py:40, engine.py:41, engine.py:327, fragility.py:101, backtest/validation.py:928.

[Medium] Public function parameters are documented but ignored in implementation.
window, lookback, and window arguments do not affect computation in three fragility functions.
Refs: fragility.py:108, fragility.py:189, fragility.py:250.

[Medium] Metadata fillna(0.0) can fabricate regime/uncertainty/volatility states.
This can reclassify unknowns as valid low-risk states and distort slices.
Ref: slicing.py:399.

[Medium] Slice mask mismatch handling is positional truncation, not index alignment.
If metadata and returns are offset, this can silently select wrong rows.
Refs: slicing.py:71, slicing.py:78.

[Medium] Date range in metrics can be inconsistent with filtered samples.
start_date/end_date come from original series even after non-finite values are removed from ret_arr.
Refs: metrics.py:56, metrics.py:121.

[Medium] Decile significance flag uses fixed |t| > 1.96 and ignores p-value output.
This is a rough asymptotic rule and can mislabel significance for small or unequal samples.
Ref: metrics.py:236.

[Medium] rolling_ic hardcodes minimum valid observations as 10 regardless of requested window.
For windows <10, function can output all NaNs even with perfect signal.
Ref: backtest/validation.py:983.

[Medium] Overall pass/fail is disconnected from subsystem completeness.
A run can be “PASS” if critical flags absent, even when important analyses failed and were suppressed.
Refs: engine.py:457, engine.py:468.

[Low] Redundant expression in walk-forward plotting.
Duplicate fallback key lookup appears to be copy/paste mistake.
Refs: visualization.py:351, visualization.py:352.

[Low] Underwater chart does not sanitize NaNs/infs before cumulative product.
Plot path can become contaminated by non-finite values.
Refs: visualization.py:190, visualization.py:193.

[Low] HTML report content is not escaped before interpolation.
If any upstream message contains markup, report HTML can be injected.
Refs: engine.py:577, engine.py:578.

[Low] Plotly report uses CDN-only JS embedding.
Offline report rendering is brittle.
Refs: visualization.py:69, visualization.py:122.

[Low] Dead/unused imports/constants indicate maintenance drift.
Example: EVAL_DECILE_SPREAD_MIN imported but unused; make_subplots imported but unused.
Refs: metrics.py:20, visualization.py:20.

Reproduced Evidence (No Guessing)

analyze_calibration with shorter confidence_scores raised: IndexError boolean index did not match indexed array....
decile_spread on ranked 100-point synthetic data returned decile counts: [9, 10, 10, 10, 10, 10, 10, 10, 10, 11].
pnl_concentration([{'pnl':1.0},{'pnl':-1.0}]) returned n_trades: 0 (incorrect).
drawdown_distribution(..., window=10) and window=500 returned identical output.
recovery_time_distribution(..., lookback=10) and lookback=500 returned identical output.
Perfectly aligned synthetic signal: aggregate IC ~1.0, but regime slice IC values remained 0.0.
Existing evaluation tests all passed (99 passed), indicating test gaps around these failure modes.
Fundamental/Conceptual Critique

The module is currently “reporting-oriented” rather than “decision-grade”; fail-open exception handling undermines trust in any “pass/fail” output.
Statistical definitions are inconsistent across metrics (e.g., annualization, significance flags), creating model-risk from metric misinterpretation.
Configuration claims (“STATUS: ACTIVE” thresholds/lookbacks) diverge from runtime behavior, reducing governance credibility.
EvaluationEngine is overly monolithic (orchestration + policy + serialization + HTML rendering), which makes correctness harder to enforce and failures easier to hide.
Testing Gaps

No test for mismatched confidence_scores length (current calibration tests only use aligned arrays).
No test for zero-net-PnL non-empty trades.
No test asserting decile bucket balance under ordered inputs.
No test asserting window/lookback parameters actually influence fragility outputs.
No test asserting slice-level IC is computed when predictions are available.




Features Folder Audit:
Findings (ordered by severity)

[P0] causality_filter="RESEARCH_ONLY" is documented but not actually enforced; the filter logic only handles "CAUSAL" and "END_OF_DAY", so "RESEARCH_ONLY" currently returns everything.
compute doc, filter logic

[P0] Unknown/unregistered features are treated as "CAUSAL" by default (fail-open), which can silently permit leakage if metadata coverage drifts.
get_feature_type

[P0] VolSpillover_Net appears to use the wrong operand: it is computed as vol_out - recv_cent instead of vol_out - vol_in. That mixes volatility and momentum-centrality semantics.
arrays defined, assignment

[P0] DTW backtracking appends (i-1, j-1) before boundary checks; when i==0 or j==0, negative indices are inserted into the path, biasing lag extraction.
DTW backtrack

[P0] Monthly macro data is described as forward-filled, but implementation forward-fills only 5 business days (ffill(limit=5)), causing most monthly periods to remain NaN.
doc claim, implementation

[P0] Benchmark loader imports are outside the try; missing modules will raise immediately and bypass graceful fallback.
imports, try starts

[P1] Feature collisions are silently dropped (duplicated() keep-first) before winsorization; this can hide schema conflicts and shadow better versions of the same signal.
dedupe, winsorizer dedupe

[P1] FeatureVersion is documented as “immutable” but is not frozen and mutates feature_names in __post_init__.
doc/dataclass, mutation

[P1] Compatibility check ignores extra current features (compatible only checks missing model features). This can mask feature-space drift.
compat logic

[P1] Mutable default argument: horizons: List[int] = [...].
definition

[P1] reversal_horizon sign-flip logic can mis-handle NaNs (NaN != 0 evaluates True), potentially flagging false reversals in sparse regions.
sign logic

[P1] Cross-asset lead-lag weights standardize blocks then replace non-finite with zero, injecting synthetic values into correlations.
standardize, usage

[P1] Eigenvalue path replaces missing returns with zeros before covariance analysis, distorting spectrum estimates.
zero fill

[P1] compute_universe has no empty-input guard; pd.concat(all_features) will fail if data is empty.
loop, concat

[P1] Indicator compute loop catches only ValueError/RuntimeError; other indicator exceptions can crash the full pipeline.
exception scope

[P1] Structural feature blocks catch broad Exception and continue, causing silent schema instability across runs/environments.
spectral catch, SSA catch, tail catch, OT catch

[P1] Macro parquet IO catches OSError/ValueError but not missing-engine import failures, so environments without parquet backends may hard-fail.
read catch, write catch

[P1] Intraday/LOB loops swallow per-date failures with pass, removing traceability for data quality and provider errors.
silent pass, silent pass, silent pass

[P2] Module-level cached indicator instances can leak state across assets/runs if any indicator is stateful; also not thread-safe.
globals/cache, getter

[P2] freq parameter in compute_lob_features is effectively unused for computation.
param

[P2] HAR volatility docs/comments are internally inconsistent with implementation (RV_weekly commentary vs 21-bar code).
comment, code

[P2] HARX docstring default for weekly window says 5 while function default is 21.
signature, doc

[P2] compute_targets has complex dual-path target generation (close-based vs ret_stream-based) with no explicit consistency checks; high regression risk if upstream return conventions change.
target logic

[P2] Expanding-quantile winsorization is computationally heavy (expanding().quantile() per column), likely to dominate runtime on wide universes.
winsorizer

[P2] Several rolling features rely on Python lambdas inside rolling apply, which are slow at scale (ofi_persistence, percentile ranks).
OFI persistence, percentile rank

[P2] Universe intraday enrichment is N-assets × N-days remote-call heavy (WRDS calls per date), no batching or cache orchestration in this layer.
loop/calls

[P3] Logging is inconsistent (print-based observability in core pipeline), making structured monitoring and alerting harder.
examples

[P3] Epsilon handling is inconsistent (_EPS exists but hardcoded 1e-10 appears in multiple files), complicating numerical-policy control.
example, example

[P3] pipeline.py is a single 1,541-line orchestration + feature-math + data-access unit, which raises maintenance and regression risk (low cohesion).
file start

[P3] README module inventory appears brittle/manual (hardcoded counts/line totals), likely to drift unless auto-generated.
summary block

Validation Gaps

Runtime validation was limited because local environment lacks pandas (ModuleNotFoundError), so this report is static-source audit only.
No tests exist in this folder to confirm intended behavior for edge cases above (especially target alignment, causality filtering modes, and cross-asset network math).




Frontend Folder Audit:
Findings (Severity-Ordered)

[P0] RadarChart is called with the wrong prop API and will not type-check/build.
References: EnsembleDisagreement.tsx:87, RadarChart.tsx:5.
EnsembleDisagreement passes categories/series, but RadarChart expects indicators/values.

[P0] FeatureDiffViewer reads previous_importance, but the declared FeatureImportance type does not include it.
References: FeatureDiffViewer.tsx:14, models.ts:37.
This is contract drift between UI and declared API types.

[P0] PaperPnLTracker uses fields not declared on PaperState and masks the mismatch with any.
References: PaperPnLTracker.tsx:16, PaperPnLTracker.tsx:19, autopilot.ts:63.
initial_capital and trade_history are used but not typed on PaperState.

[P0] BacktestResults uses p-value fields not declared on BacktestResult.
References: BacktestResults.tsx:98, backtests.ts:2.
Fields like sharpe_pvalue, spa_pvalue, dsr_pvalue, ic_pvalue are referenced without type support.

[P1] CSV export is vulnerable to spreadsheet formula injection.
Reference: useCSVExport.ts:14.
Values beginning with =, +, -, @ are exported raw.

[P1] Dynamic URL path/query values are concatenated without encoding.
References: endpoints.ts:35, endpoints.ts:53, useData.ts:51, useData.ts:75, useBacktests.ts:18, useSignals.ts:9.
This risks malformed requests and edge-case injection through IDs/tickers/params.

[P1] usePatchConfig invalidates ['config'] but not ['config-status'], leaving stale UI state.
References: usePatchConfig.ts:11, useConfig.ts:16.

[P1] ScatterChart sets symbolSize to an array, which is not a valid ECharts scatter symbolSize shape.
Reference: ScatterChart.tsx:50.
This can silently render wrong marker sizing.

[P1] JobMonitor can invoke onComplete repeatedly on subsequent renders.
Reference: JobMonitor.tsx:14.
No dedupe guard for already-fired completion callbacks.

[P1] StatusBar bypasses the shared API client and hardcodes environment assumptions.
References: StatusBar.tsx:30, StatusBar.tsx:97.
Hardcoded /api/v1/... and localhost (line 8000) increase drift risk across envs.

[P1] External remote script injection from TradingView has no integrity/CSP hardening in-app.
References: TradingViewWidget.tsx:43, TradingViewWidget.tsx:46.
This is a material supply-chain/runtime trust boundary.

[P1] API error handling discards structured server error envelopes on non-2xx responses.
References: client.ts:29, client.ts:31.
Only raw text is thrown, losing typed error metadata and diagnostics.

[P1] API client forces Content-Type: application/json on all requests including GET.
Reference: client.ts:24.
Unnecessary headers can affect caches/proxies and preflight behavior.

[P1] useDownsample can divide by zero with unsafe threshold values.
References: useDownsample.ts:11, useDownsample.ts:27.
No guard for threshold <= 2 / zero bucket spans.

[P1] DataProvenanceBadge assumes meta.warnings is always present and array-like.
Reference: DataProvenanceBadge.tsx:49.
A backend omission would crash this render path.

[P2] useTickerIndicators cache key is order-sensitive (A,B vs B,A) and causes avoidable cache misses.
References: useData.ts:94, useData.ts:96.

[P2] No lint/test scripts are defined; quality gates are weak.
Reference: package.json:6.
Only dev, build, preview, typecheck exist.

[P2] Compiler strictness is diluted by disabling unused checks.
Reference: tsconfig.json:15.
noUnusedLocals and noUnusedParameters are both false.

[P2] ErrorBoundary fallback uses hard navigation (<a href="/">) instead of SPA navigation.
Reference: ErrorBoundary.tsx:48.
Causes full reload and state loss.

[P2] Style token inconsistency: accent-yellow is used but not declared in tokens.
References: tokens.css:25, StatusBar.tsx:50, CacheStatusPanel.tsx:7.
Current code relies on fallback literals, reducing theme coherence.

[P2] Endpoint inventory is internally inconsistent (comment + dead constant).
References: endpoints.ts:1, endpoints.ts:70.
DATA_TICKER_INDICATORS_BATCH is declared but unused.

[P2] Version strings are hardcoded in multiple UI spots rather than sourced from one place.
References: Sidebar.tsx:157, StatusBar.tsx:102, package.json:4.

[P2] Multiple components mutate inline styles imperatively for hover states.
References: Sidebar.tsx:113, MetricCard.tsx:19.
This is brittle and harder to maintain/test than CSS classes.

[P2] Several key feature modules are monolithic and mix data orchestration with rendering.
References: RegimeTab.tsx:180, RiskTab.tsx:17, BenchmarkChartGrid.tsx:58.
This increases regression risk and test difficulty.

[P2] Bundle strategy remains heavy due simultaneous use of ECharts, Plotly, and Lightweight Charts.
References: package.json:16, package.json:18, package.json:20, vite.config.ts:24.
Manual chunking helps, but library overlap is still substantial.

[P3] tsconfig.tsbuildinfo is tracked; this is generated state and creates noisy diffs/churn.
Reference: tsconfig.tsbuildinfo.

Conceptual / Fundamental Critique

There is persistent API-contract drift between declared frontend types and usage sites; this indicates missing schema-driven type generation or contract tests.
UI business logic is spread across pages with repeated formatting/constants, indicating weak domain centralization (harder to evolve safely).
Cross-environment configuration is not centralized (hardcoded URLs/versions), which will keep causing deployment/config regressions.
Observability and failure ergonomics are uneven (some paths use shared envelopes, others bypass and swallow errors).
Security posture is inconsistent (CSV export + third-party script trust + weak request canonicalization).
Coverage / Constraints

I audited all first-party source/config files under src plus root app configs (index.html, package.json, vite.config.ts, tsconfig.json, Tailwind/PostCSS config), and reviewed lock/build artifacts for hygiene signals.
I could not execute typecheck, lint, or runtime tests because npm is unavailable in this environment (command not found).
Findings above are therefore static-source grounded only (no guessed code paths, no speculative runtime traces).
Open Questions

Is backend intentionally returning fields not represented in frontend types (previous_importance, *_pvalue, initial_capital, trade_history), or are these accidental drifts?
Is TradingView embed required in production, or can it be isolated behind a stricter security boundary/feature flag?




Indicators Folder Audit:

Findings (Ordered by Severity)

[P0] Aroon is mathematically inverted (recent highs/lows get low scores instead of high scores). The implementation uses ((period - np.argmax(x)) / period) * 100, which gives 0 when the most recent bar is the high; standard Aroon Up should be 100 in that case. This flips signal interpretation.
indicators.py:459
indicators.py:463

[P0] PivotHigh/PivotLow are non-causal due to centered rolling windows (center=True), which require future bars. That introduces look-ahead leakage for any backtest/live model expecting causal features.
indicators.py:1031
indicators.py:1067

[P1] compute_expected_shortfall uses np.partition(seg, k)[:k]; this should typically partition at k-1 for the worst k observations. Current code can include a non-tail element and bias ES upward.
tail_risk.py:105
tail_risk.py:107

[P1] RegimePersistence silently treats NaN states as 0 (because boolean comparison then cast to int), so missing data is counted as a regime instead of being skipped. The np.isnan(current_state) check is ineffective because current_state is int.
indicators.py:2719
indicators.py:2728

[P1] Alias mechanism is dead code: INDICATOR_ALIASES exists but create_indicator() ignores it and only checks get_all_indicators(). This is a functional API inconsistency.
indicators.py:2885
indicators.py:2899

[P1] Package API fragmentation: analyzers in eigenvalue.py, ssa.py, spectral.py, tail_risk.py, ot_divergence.py are not exported from package root, not in registry, and not reachable through create_indicator(). They are effectively isolated from the primary API surface.
init.py:7
eigenvalue.py:20
ssa.py:15
spectral.py:15
tail_risk.py:15
ot_divergence.py:16

[P2] Percentile indicators divide by len(x) while comparing against x[:-1], so they cannot reach true 100 percentile and are slightly biased low.
indicators.py:704
indicators.py:735
indicators.py:762

[P2] CandleDirection classifies doji candles (Close == Open) as -1 (bearish), creating systematic downside bias.
indicators.py:632

[P2] Value-area/POC calculations start after 5 bars regardless of configured period, producing early-window outputs from insufficient history and inconsistent statistical meaning.
indicators.py:1497
indicators.py:1540
indicators.py:1585

[P2] Multiple indicators can emit inf/-inf from divide-by-zero without sanitization (e.g., flat ranges, zero moving averages, zero VA range).
indicators.py:102
indicators.py:258
indicators.py:300
indicators.py:900
indicators.py:1212
indicators.py:1661

[P2] Many “canonical” indicator names use non-canonical formulas (ATR/RSI/ADX use simple rolling means instead of Wilder smoothing), which can materially differ from textbook/reference implementations and break comparability.
indicators.py:56
indicators.py:148
indicators.py:434

[P2] Broad exception swallowing in advanced estimators hides numerical/data failures and can silently degrade feature quality.
indicators.py:2337
indicators.py:2398
indicators.py:2480
indicators.py:2553

[P2] Time alignment is inconsistent across modules: many indicators use rolling windows including current bar, while analyzer modules typically use [i-window:i] (excluding current). This creates one-bar semantic drift when features are combined.
indicators.py:55
eigenvalue.py:140
ssa.py:118
spectral.py:97
tail_risk.py:66
ot_divergence.py:171

[P2] Parameter validation is uneven/missing in core indicators (e.g., non-positive periods, invalid bin counts, invalid GARCH parameter region) and can lead to runtime errors or invalid math.
indicators.py:36
indicators.py:1567
indicators.py:1986
eigenvalue.py:42

[P2] EigenvalueAnalyzer does not validate min_assets >= 2 or regularization bounds; edge inputs can create degenerate/scalar correlation behavior.
eigenvalue.py:43
eigenvalue.py:44

[P3] compute_all() paths duplicate logic instead of reusing single-metric methods/helpers, increasing drift risk during future maintenance.
eigenvalue.py:297
ssa.py:254
spectral.py:255

[P3] Unused imports (pandas) in analyzer modules and unused typing imports in indicators.py add noise and suggest linting gaps.
eigenvalue.py:16
ot_divergence.py:13
spectral.py:12
ssa.py:12
tail_risk.py:12
indicators.py:10

[P3] PivotHigh docstring claims “days since last break,” but implementation returns binary breakout flags.
indicators.py:1010
indicators.py:1039

[P3] Research-risk feature concern: Beast666* indicators are highly hypothesis-specific and may increase overfitting risk without strong statistical validation controls.
indicators.py:1714

[P3] Repository hygiene: Python cache artifacts are present in versioned folder (__pycache__ files), which should generally be ignored.
/Users/justinblaise/Documents/quant_engine/indicators/pycache/init.cpython-312.pyc

[P3] No test suite exists in this folder (no test_*.py/*_test.py files), so correctness and numerical stability are unguarded against regressions.

Reference-Trace Coverage (Completed)

Verified __init__.py imports and __all__ are internally consistent (93 imported symbols, 93 exported symbols, all resolved to definitions in indicators.py).
init.py:7
init.py:59

Verified get_all_indicators() registry maps 91 keys to real classes, no broken class references.
indicators.py:2744

Traced internal class-to-class dependencies (e.g., NATR -> ATR, MACDHistogram -> MACD/MACDSignal, ValueAreaPosition -> VAH/VAL, etc.) directly to source definitions.

Open Questions / Assumptions

Should all features be strictly causal for production trading? If yes, PivotHigh/PivotLow need redesign or explicit delayed labeling semantics.
Do you require canonical TA-equivalent outputs (TA-Lib parity), or are custom variants acceptable?
Should analyzer modules (SSA, Spectral, Eigenvalue, TailRisk, OT) be first-class in package API/registry?
Do you want strict NaN/inf policies (drop, cap, propagate, or fail-fast) standardized across all indicators?
Execution Limits

No code was changed.
Syntax compilation succeeded with py_compile.
Full runtime validation was blocked in this environment because numpy/pandas are not installed in the sandbox interpreter.




Kalshi folder audit:

Critical Findings

Threshold-direction inference can misclassify contracts because it matches raw substrings like "ge"/"le" in free text, which can trigger on unrelated words and invert semantics in distribution construction. distribution.py:164 distribution.py:167
Event-to-market fallback merge can mis-assign market_id because it fills from a separately merged frame by index alignment, not keyed alignment. events.py:134 events.py:139
Storage writes are destructive for partial rows: INSERT OR REPLACE plus column-subset insertion can null out previously stored fields not present in the incoming row. storage.py:153 storage.py:160
Walk-forward capacity metrics are computed on per-row traces (including multi-horizon duplicates), so event frequency/capacity constraints can be materially overstated. walkforward.py:451 walkforward.py:208
Regime tagging drops valid zero values (0 becomes np.nan) due to value or np.nan, causing neutral regimes to be mislabeled as unknown. regimes.py:80
For threshold markets with unresolved direction, the code still computes moments from p_raw and returns non-NaN distribution stats, which are not semantically valid. distribution.py:683 distribution.py:721
Bin-validation output is computed but not enforced in quality_low gating, so structurally invalid bins can still pass through as modeled features. distribution.py:542 distribution.py:755
mapping_version ordering is lexicographic in as-of mapping retrieval, which is unsafe for versions like v10 vs v2. storage.py:633
High Findings

current_version() returns the last row of a sorted dataframe, not a true global latest version; result can be wrong depending on event ordering. mapping_store.py:58 mapping_store.py:63
Passphrase is passed on the OpenSSL command line (-passin pass:...), exposing secrets in process listings. client.py:222 client.py:241
Retry loop treats many permanent failures (e.g., JSON shape/signing/runtime issues) as retryable, which can hide root causes and amplify API load. client.py:471
Unknown environment values silently coerce to "demo" instead of failing fast, creating high risk of accidental wrong-environment execution. client.py:23 client.py:28
build_asset_response_labels is not asset-keyed and scans one shared price table, so multi-asset inputs can produce cross-asset label contamination. events.py:469 events.py:494
compute_microstructure can raise on tz-aware asof_ts because pd.Timestamp(asof_ts, tz="UTC") is invalid when tz already exists. microstructure.py:62
compute_quality_dimensions uses volumes or []/open_interests or []; numpy arrays here can raise ambiguous-truth errors. quality.py:133
Promotion wiring maps regime_positive_fraction to event_regime_stability; this is a metric-contract mismatch and can distort candidate ranking. promotion.py:167 promotion_gate.py:311
support_is_ordered is evaluated after sorting, so it cannot detect original ordering issues and is effectively always true for finite bins. distribution.py:218 distribution.py:247
violated_constraints_post is not an actual post-cleaning count; it echoes pre-count unless adjustment is exactly zero. distribution.py:804
Distribution generation scales poorly (markets x snapshots x per-snapshot reconstruction), with repeated full-frame filtering/merging; this will degrade sharply at production quote volumes. provider.py:524 distribution.py:909
Provenance table key (market_id, asof_date, endpoint) plus constant market_id="*" causes same-day overwrite, losing run history granularity. storage.py:418 provider.py:198
Medium / Fundamental Design Critiques

Hard-gate quality function exists but is not used in the distribution build path; soft flags are produced without a centralized enforce/fail policy. quality.py:164 distribution.py:745
quality_as_feature_dict duplicates the same proxy into both volume_proxy and oi_proxy, which is semantically misleading to downstream models. quality.py:204
Mapping as-of retrieval does not resolve one canonical mapping row per event at query time; downstream joins can duplicate rows per event/horizon. storage.py:627 events.py:125
Label builder is O(events × outcomes) via per-event dataframe filtering loops; this will become expensive for larger calendars. events.py:395
Distance-feature generation is nested per-market/per-lag/per-row with per-row .loc writes, which is computationally heavy and non-vectorized. distribution.py:838
KALSHI_ENV is hard-coded demo in config and provider consumes that constant directly; prod switching relies on code/config edits rather than runtime env controls. config.py:68 provider.py:83
Several public APIs are currently orphaned (defined but never referenced anywhere in repo), indicating architectural drift and unclear integration intent. disagreement.py:32 microstructure.py:31 events.py:325
Exception handling in provider sync paths swallows operational errors and returns empty outputs, which reduces observability and can mask ingestion failures. provider.py:99 provider.py:128
Storage class has no explicit close/dispose lifecycle for DB connections, increasing risk of open handles in long-lived processes/tests. storage.py:44
EventTimestampMeta exists but is not used in event-panel/label pipelines, so timestamp lineage intent is not enforced end-to-end. events.py:14
Walk-forward uses deprecated lowercase time unit ("d") producing pandas warnings; future pandas versions can break current configs. walkforward.py:338
README module inventory is stale vs current line counts/functions, reducing trust in package documentation as a control surface. README.md:12
Test / Verification Gaps

Major operational modules are untested (client, provider, storage, pipeline, router, options, promotion, mapping_store), leaving high-risk paths without regression coverage. tests/test_distribution.py:1 tests/test_walkforward_purge.py:1
Signature test is validity-oriented, not true known-answer deterministic verification (no expected signature bytes asserted). tests/test_signature_kat.py:72
No tests assert correctness of mapping version resolution/current version behavior. mapping_store.py:56
No tests exercise failure modes (network errors, malformed payloads, DB conflicts, partial upserts). client.py:423 storage.py:143
Execution Evidence

Full kalshi test run in project venv: .venv/bin/python -m pytest -q kalshi/tests from /Users/justinblaise/Documents/quant_engine -> 45 passed, 3 skipped.
Observed warnings include pandas deprecation for lowercase "d" timedelta units in walk-forward paths.
No code was modified.




Models Folder Audit:
Comprehensive audit completed with full source tracing across the entire /models folder plus referenced upstream code (config, config_structured, features/pipeline, validation/preconditions, run_retrain, backtest/engine, regime/detector, risk/position_sizer). No code was changed.

Findings (ordered by severity)

[P0] Regime suppression is semantically inconsistent and can suppress the wrong regime.
predictor force-zeros confidence for regime == 2 while labeling it as high-volatility, but canonical mapping defines 2 = mean_reverting, 3 = high_volatility. This can suppress mean-reverting signals unintentionally and conflicts with other modules’ regime semantics.
Refs: predictor.py, predictor.py, config.py, regime/detector.py, risk/position_sizer.py

[P0] Confidence calibration is trained on a different signal than what inference calibrates.
Training calibrator input is normalized abs(prediction); inference calibrates a composite confidence score derived from holdout/CV/regime metrics. That is a domain mismatch, so calibrated confidence is not statistically meaningful.
Refs: trainer.py, trainer.py, predictor.py, predictor.py

[P0] walk_forward_select has hard runtime failure modes.
It keeps rows with partial NaNs (any(axis=1)), then fits GradientBoostingRegressor, which raises on NaNs. It can also reach np.nanargmax with all-NaN scores and raise ValueError: All-NaN slice encountered.
Refs: walk_forward.py, walk_forward.py, walk_forward.py

[P1] Retrain quality gate ignores negative OOS Spearman.
Condition requires oos_spearman > 0 before checking threshold, so very bad (negative) model quality does not trigger retraining.
Refs: retrain_trigger.py

[P1] Governance promotion math can promote a worse challenger when current score is negative.
Relative threshold is current_score * (1 + min_relative_improvement), which becomes more negative when current score is negative; a more-negative challenger can still satisfy score > threshold.
Refs: governance.py, governance.py

[P1] Distribution-shift retrain logic is effectively disconnected from production retrain flow.
RetrainTrigger.check_shift() exists, but check() does not call it, and run_retrain uses check() only.
Refs: retrain_trigger.py, retrain_trigger.py, run_retrain.py

[P1] Black-Scholes zero-vol branch is financially incorrect for ITM options.
For sigma <= 0 and T > 0, price returns 0.0 instead of deterministic discounted intrinsic value.
Refs: iv/models.py, iv/models.py

[P1] Causality enforcement is allow-by-default for unknown features.
Unknown feature names default to CAUSAL; predictor’s truth-layer check only blocks explicit RESEARCH_ONLY. Untagged new features bypass protection.
Refs: features/pipeline.py, features/pipeline.py, predictor.py

[P1] Champion loading ignores caller model_dir.
Champion resolution constructs ModelGovernance() with default registry path, not one tied to predictor’s self.model_dir.
Refs: predictor.py, governance.py, config.py

[P2] PSI implementation drops out-of-range mass from current distribution.
Current histogram uses fixed reference bins; samples outside bin range are excluded, so frequency mass can sum to <1, making PSI numerically inconsistent.
Refs: shift_detection.py, shift_detection.py, shift_detection.py

[P2] Calibrator “temporal split” claim is not guaranteed.
Calibration holdout is X.iloc[-n_holdout:] without sorting by date in _fit_calibrator; if input order is non-temporal, split is not temporal.
Refs: trainer.py, trainer.py

[P2] versioned=False training still writes global artifacts and prints unconditionally.
Non-versioned path writes to global MODEL_DIR, creating side effects and coupling tests/runtime to a shared path.
Refs: trainer.py, trainer.py, config.py

[P2] Retrain trade-count logic compares timestamps as strings, not datetimes.
Lexicographic comparison is format-sensitive and brittle with timezone/format drift.
Refs: retrain_trigger.py

[P2] Registry/metadata writes are non-atomic and weak to corruption/races.
Direct read/write JSON with no lock, no temp-file rename, and limited corruption handling.
Refs: governance.py, governance.py, versioning.py, versioning.py, retrain_trigger.py, retrain_trigger.py

[P3] Online drift detection updates stats before z-test, reducing sensitivity.
Current sample is blended into mean/variance before computing z-score for that same sample.
Refs: online_learning.py, online_learning.py

[P3] Cross-sectional fallback masks missing predictions as neutral ranks.
For groups with <2 valid values, all rows (including NaN predictions) are assigned rank 0.5 and z-score 0.
Refs: cross_sectional.py, cross_sectional.py

[P3] Static quality debt: unresolved lint-level correctness/hygiene issues.
Ruff F/B flags include undefined type name in annotation (pd), unused variables, and dead imports in audited files.
Refs: retrain_trigger.py, trainer.py, cross_sectional.py

Open Questions / Ambiguities To Resolve

Should regime suppression target code 2 or 3? Current config labels 2 as mean-reverting and 3 as high-volatility, but predictor comment and hard suppression logic are inconsistent.
Should confidence calibration map |prediction| or model confidence probability? Current train/infer semantics differ.
Is walk_forward.py intended for production use? It is exported but appears unintegrated and currently fragile.
Validation Performed

Ran targeted model-related tests: 184 passed (test_training_pipeline_fixes, test_system_innovations, test_loader_and_predictor, test_iv_arbitrage_builder, test_panel_split, test_health_ensemble_disagreement, test_truth_layer).
Ran static bug lint (ruff --select F,B): 33 findings (mostly hygiene, plus a few correctness-smell items).
Executed direct repro probes for key defects (negative Spearman retrain gate, governance negative-score promotion, walk-forward NaN/All-NaN failure, zero-vol Black-Scholes pricing behavior, and unknown-feature causality default).
Coverage Note

All 17 files in /Users/justinblaise/Documents/quant_engine/models were reviewed line-by-line, including:
__init__.py, README.md, calibration.py, conformal.py, cross_sectional.py, feature_stability.py, governance.py, neural_net.py, online_learning.py, predictor.py, retrain_trigger.py, shift_detection.py, trainer.py, versioning.py, walk_forward.py, __init__.py, models.py.




Regime Folder Audit:
Audit Findings (No Code Changes Made)

P0 (High-Risk Functional/Design Defects)

method="ensemble" is documented but not actually supported by detect_full; it falls through to rule-based detection.
Evidence: detector.py:93, detector.py:563, detector.py:569.
Impact: API behavior is inconsistent and silently wrong for callers expecting explicit ensemble mode.

_apply_min_duration confidence tie-break logic is effectively broken; both sides use the same score, so short runs always bias left when both neighbors exist.
Evidence: detector.py:166, detector.py:167, detector.py:170.
Observed behavior: changing confidence did not change replacement direction in a repro case.

Jump model fallback behavior contradicts implementation comments/docs: PyPI jump failures (including short series) fall back to rules, not legacy jump model.
Evidence: jump_model_pypi.py:13, detector.py:326, detector.py:333.
Impact: materially different model behavior than advertised.

ShockVector transition schema is hardcoded to (4,4), but HMM state count is auto-selectable (2–6), so valid detector outputs can be marked invalid and still returned.
Evidence: config_structured.py:117, detector.py:804, shock_vector.py:267, detector.py:808, detector.py:812.
Observed behavior: produced invalid ShockVector with (3,3) transition matrix.

HMM observation standardization uses full-series mean/std, which introduces temporal leakage risk for historical regime labeling/backtest workflows.
Evidence: hmm.py:597, hmm.py:602.
Impact: earlier timestamps are normalized using future data.

P1 (Material Reliability/Architecture Issues)

“Online” BOCPD path in detect_with_shock_context re-runs full batch update each call (and batch resets internal state), so it is not incremental online inference.
Evidence: detector.py:753, bocpd.py:432.
Impact: unnecessary compute and mismatch with stated design intent.

detect_with_shock_context has no empty-input guard and can throw IndexError.
Evidence: detector.py:794, detector.py:796.
Observed behavior: empty DataFrame repro throws index -1 is out of bounds.

Config knobs are declared as active but unused in this package path (REGIME_ENSEMBLE_CONSENSUS_THRESHOLD, BOCPD_CHANGEPOINT_THRESHOLD, REGIME_JUMP_MODE_LOSS_WEIGHT).
Evidence: detector.py:37, detector.py:42, jump_model_pypi.py:91.
Impact: operational tuning can be misleading/non-functional.

compute_shock_vectors diverges from detector semantics by proxying uncertainty as 1-confidence and hardcoding output metadata (schema_version="1.0", ensemble_model_type="hmm").
Evidence: shock_vector.py:409, shock_vector.py:475, shock_vector.py:487.
Impact: two paths producing ShockVectors can disagree in meaning.

Import path fragility: importing regime transitively requires top-level absolute config_structured import resolution.
Evidence: init.py:3, detector.py:24, config.py:29.
Observed behavior: test collection failed without PYTHONPATH=...

RegimeConsensus.compute_consensus does not validate regime label bounds; out-of-range values dilute percentages silently.
Evidence: consensus.py:104, consensus.py:119, consensus.py:121.

ConfidenceCalibrator.calibrate wraps invalid regime IDs with modulo rather than rejecting, which can silently remap wrong labels.
Evidence: confidence_calibrator.py:181.

OnlineRegimeUpdater is tightly coupled to a private HMM method and uses uniform prior for first cached step rather than model priors.
Evidence: online_update.py:110, online_update.py:169.

P2 (Quality, Maintainability, Test Completeness)

README inventory is stale/inaccurate (e.g., LOC and module line counts).
Evidence: README.md:12, README.md:29.
Observed total current line count: 4540 (from wc -l over package files + README).

Test gap: no direct test for RegimeDetector(method="ensemble") path despite documented support.
Evidence: no matches for RegimeDetector(method="ensemble") in tests; contrast with ensemble tests using method="hmm": test_regime_integration.py:150.

Test gap: structural ShockVector validity checks are mostly exercised with method="rule" and do not cover variable-state HMM transition matrices in detect_with_shock_context.
Evidence: test_structural_state_integration.py:179, test_structural_state_integration.py:205.

Execution-Based Validation Performed

Parsed all Python files with ast successfully (13/13 files).
Ran targeted lint (ruff --select F,B,E9): found unused/dead symbols and annotation/name issues (17 findings).
Ran regime-related tests with proper import context: 350 passed, 2 warnings plus additional structural/conceptual tests 34 passed.
Open Questions

Should ShockVector transition data be canonicalized to 4 semantic regimes, or should validator accept variable KxK matrices when HMM auto-select is enabled?
Is method="ensemble" intended public API, or should docs/config stop advertising it and rely only on method="hmm" + ensemble flag?
For short series with PyPI jump model unavailable, should fallback be legacy jump (as documented) instead of rule-based?




Risk Folder Audit:

Critical Findings

Short positions can bypass core risk constraints in PortfolioRiskManager. Single-name, gross, and sector checks are implemented on signed values, so a -50% position can pass with no violation and even produce negative utilizations. See portfolio_risk.py#L348, portfolio_risk.py#L356, portfolio_risk.py#L481, portfolio_risk.py#L488, portfolio_risk.py#L496.
Drawdown escalation order can suppress liquidation in truly critical events. A -20% day enters CAUTION, not CRITICAL, because daily/weekly branches execute first. See drawdown.py#L121, drawdown.py#L128.
replay_with_stress_constraints does not apply scenario shock parameters to constraint math, so scenario severity is effectively ignored for pass/fail/utilization. Only metadata columns differ. See constraint_replay.py#L88, constraint_replay.py#L133.
optimize_portfolio fallback returns unconstrained equal weights after SLSQP failure, which can violate max_position and other constraints. See portfolio_optimizer.py#L257, portfolio_optimizer.py#L262.
correlation_stress_test assumes weight order matches covariance order by dict insertion; it validates only length, not asset mapping, so silently wrong risk numbers are possible. See stress_test.py#L435, stress_test.py#L437.
High-Risk Modeling / Logic Defects

compute_constraint_utilization never returns a "correlation" key, but downstream replay expects it; correlation utilization is effectively forced to 0.0. See portfolio_risk.py#L460, constraint_replay.py#L122.
Portfolio beta in PortfolioRiskManager is not normalized by total weight and can collapse incorrectly for net-short books. See portfolio_risk.py#L740, portfolio_risk.py#L755.
Hard-stop spread buffer is calculated but not used in the trigger condition (trigger uses raw unrealized return threshold). See stop_loss.py#L152, stop_loss.py#L153.
FactorExposureManager uses absolute weights for exposure aggregation, so short books can report long-like beta exposure. See factor_exposures.py#L139, factor_exposures.py#L148.
PositionSizer.update_kelly_bayesian accumulates regime wins/losses with +=, so repeated calls over full history double-count. See position_sizer.py#L929.
PositionSizer does not have a clean “no-trade” path under bad inputs because vol/ATR fall back to min_position and final clip enforces >= min_position. See position_sizer.py#L733, position_sizer.py#L752, position_sizer.py#L301.
correlation_stress_test’s diversification_benefit_lost formula computes normal diversification benefit, not “benefit lost under stress”. See stress_test.py#L482.
constraint_replay also uses signed gross/single/sector math, compounding the long/short issue in diagnostics. See constraint_replay.py#L110, constraint_replay.py#L113, constraint_replay.py#L119.
Conceptual / Structural Critiques

Attribution combines betas/loadings with cumulative returns, which is not consistent with period-by-period linear factor decomposition. See attribution.py#L136, attribution.py#L144, attribution.py#L156.
Rolling attribution omits regression intercept when decomposing each period return. See attribution.py#L245, attribution.py#L261.
_estimate_beta fallback comment says “market-neutral fallback” but returns 1.0 (market-like). See attribution.py#L36.
Cost participation uses weight fraction divided by dollar ADV, mixing units and likely mis-scaling impact. See cost_budget.py#L68.
Partial rebalance renormalization modifies all weights, including untraded ones, which can hide residual cash and distort execution intent. See cost_budget.py#L192.
Exposure monitors are mostly self-referential (z-scoring against current portfolio cross-section), reducing signal quality for true external factor drift. See factor_exposures.py#L150, factor_monitor.py#L176.
FactorExposureMonitor marks uncomputed exposures as in-bounds pass (value=0.0), suppressing data-quality alerts. See factor_monitor.py#L235.
Regime covariance cache key is only regime ID, not (regime, ticker-set, lookback), so stale/misaligned covariance reuse is possible across calls. See portfolio_risk.py#L614.
size_position_paper_trader accepts regime_state but ignores it for blend mapping by forwarding regime=None. See position_sizer.py#L345, position_sizer.py#L427.
Turnover budget units are inconsistent with config cap (annualized_turnover computed as fraction/year, cap set to 500.0), making enforcement weak/non-binding in practice. See position_sizer.py#L644, position_sizer.py#L654, config.py#L385.
UniverseConfig does not detect duplicate tickers across sectors; later entries silently overwrite earlier ones. See universe_config.py#L124.
README inventory is stale versus current codebase metrics and module sizes, reducing trust in package docs. See README.md#L12.
Coverage Gaps (important)

I found no direct tests for decompose_returns or compute_attribution_report arithmetic validity under realistic factor structures.
I found no direct tests for run_stress_scenarios and run_historical_drawdown_test correctness of scenario math and episode logic.
Existing tests for spread buffer focus on compute_initial_stop, not evaluate hard-stop trigger semantics (where the mismatch exists).
Correlation stress tests validate vol increase, but not asset-name/covariance alignment safety.
Verification Performed

Read every file in /risk line-by-line plus referenced sources: config.py, universe.yaml, uncertainty_gate.py.

Ran targeted test suites (read-only): 275 passed total across risk-relevant files (test_portfolio_layer, test_spec_p01/p02/p03/p04, test_risk_improvements, test_regime_covariance_wiring, test_gics_sector_wiring, etc.).

Reproduced multiple defects with small runtime checks (short-position bypass, critical drawdown suppression, scenario-invariant replay, optimizer fallback constraint break, weight-order sensitivity in correlation stress).







